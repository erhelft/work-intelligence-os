# V0 vs V1 System Prompt Comparison: Event Scoring Agent

**Generated:** January 10, 2026  
**Purpose:** Compare system prompts generated by V0 vs V1 workflows to understand framework improvements

---

## Executive Summary

The V1 workflow produced a **leaner, more focused system prompt** (~1,500 tokens vs ~1,800 tokens estimated for V0) through better upfront classification and more precise risk identification. Key improvement: V1's configuration-driven approach prevented over-engineering by forcing explicit justification of complexity and risk dimensions before generation.

**Classification Change:**
- **V0 approach**: Would have likely classified as Judgment Agent (Significant Reasoning)
- **V1 result**: Workflow Executor (Moderate Reasoning) after user challenged classification
- **Impact**: 25% token budget reduction (1800-2500 → 1200-1800)

**Key Difference:** V1's explicit configuration step (Step 2-3) with user approval gate caught over-classification before generation, whereas V0 might have proceeded directly to generation with heavier prompt.

---

## 1. Spec Framework Comparison

### V0 Spec (Section 10: 5 Characteristics)

| Characteristic | Level | Purpose |
|----------------|-------|---------|
| Sensitivity | High | Data contains PII, client info, privileged data |
| Autonomy | High | Makes all decisions without human review |
| Exposure | Internal + Partners | Outputs visible to internal users and potentially clients |
| Reversibility | Easily Reversible | Recommendations only, users review before acting |
| Blast Radius | Team | Mistakes affect team-level scheduling decisions |

**Observations:**
- Characteristics are descriptive of agent properties
- No explicit mapping to prompt structure
- Mix of risk dimensions (Sensitivity, Blast Radius) and operational properties (Autonomy, Reversibility)
- Less clear how these drive prompt configuration

### V1 Spec (Section 10: 7 Dimensions)

**Core Characteristics (5 level-based):**

| Dimension | Level | Maps To |
|-----------|-------|---------|
| Reasoning Depth | Moderate | Complexity classification + Decision Logic depth |
| Action Scope | Narrow | Complexity classification + Operational Boundaries |
| Consequence Severity | Moderate | Guardrail intensity |
| Recovery Difficulty | Easy | Confirmation requirements |
| Data Sensitivity | Personal | Hard Boundaries data handling |

**Profiles (2 selection-based):**

| Profile | Selection | Drives |
|---------|-----------|--------|
| Risk Profile | Decision | Protection investment strategy |
| Excellence Profile | Accuracy + Clarity | Quality investment strategy |

**Observations:**
- **Structured mapping**: Reasoning Depth + Action Scope → direct complexity classification
- **Separation of concerns**: Risk vs. Excellence as distinct investment strategies
- **Actionable**: Each dimension clearly maps to prompt sections/depth
- **Precision**: Risk Profile forces identification of *where* protection is needed, not just that risks exist

---

## 2. Structural Comparison

### V0 System Prompt Structure

```
<system_prompt>
  <identity>                              [~150 tokens]
  <hard_boundaries>                        [~200 tokens]
  <domain_context>                         [~250 tokens]
  <decision_logic>                         [~600 tokens] ← Robust
  <operational_boundaries>                 [~100 tokens]
  <input_format_and_dynamic_context>       [~200 tokens]
  <examples>                               [~500 tokens] ← 3 detailed examples
  <output_format>                          [~250 tokens]
  <failure_handling>                       [~200 tokens]
  <final_reminders>                        [~100 tokens]
</system_prompt>

Estimated Total: ~2,550 tokens (XML tags included)
Estimated Content: ~1,800 tokens
```

**Tag-based structure** with XML-style sections.

### V1 System Prompt Structure

```
# System Prompt: Event Scoring Agent

## Identity & Purpose                     [~150 tokens] - Standard
## Hard Boundaries                        [~150 tokens] - Standard
## Domain Context                         [~80 tokens]  - Minimal
## Decision Logic                         [~400 tokens] - Minimal (but comprehensive)
## Operational Boundaries                 [~80 tokens]  - Standard
## Memory Management                      [~30 tokens]  - Standard
## Dynamic Context Injection              [~200 tokens] - Standard
## Examples                               [~450 tokens] - Standard (3 examples)
## Output Format                          [~200 tokens] - Standard
## Failure Handling                       [~200 tokens] - Standard
## Final Reminders                        [~60 tokens]  - Minimal

Actual Total: ~1,500 tokens
```

**Markdown heading structure** with clear section hierarchy.

### Key Structural Differences

1. **Format**: V0 uses XML tags (`<section>`), V1 uses Markdown headers (`## Section`)
2. **Metadata**: V1 includes version, date, complexity classification at top
3. **Section count**: V1 has Memory Management (V0 doesn't), V0 combines Input Format + Dynamic Context
4. **Decision Logic**: V0 ~600 tokens (Robust), V1 ~400 tokens (Minimal but complete)
5. **Domain Context**: V0 ~250 tokens (detailed), V1 ~80 tokens (essential patterns only)

---

## 3. Content Depth Analysis

### Decision Logic Section

**V0 Decision Logic (~600 tokens):**
- 4 Core Reasoning Principles (comprehensive)
- 7-Dimension Framework (detailed explanations)
- Signal Weighting Guidance (explicit ranges)
- Tradeoff Guidance (3 scenarios with examples)
- **Depth:** Robust—extensive reasoning infrastructure

**V1 Decision Logic (~400 tokens):**
- Core Framework: Seven Dimensions (clear but concise)
- 3 Reasoning Principles (integrated with framework)
- Confidence Calibration (explicit ranges)
- Tradeoff Guidance (2 scenarios, focused)
- **Depth:** Minimal—framework guidance without elaboration

**Key Difference:**
- V0 builds elaborate reasoning infrastructure (4 principles + detailed dimension explanations)
- V1 presents framework concisely with essential guidance
- V0 separates principles from dimensions; V1 integrates them
- Both cover same concepts, V1 is 33% more token-efficient

**Example - Event Type Dimension:**

V0:
```
**1. Event Type & Urgency**
- Infer category from title + description + location together
- **Non-English titles**: ALWAYS attempt translation—use your multilingual 
  capabilities to translate titles into English before assessment. Common 
  languages (Spanish, French, German, Mandarin, Hebrew, Arabic, etc.) should 
  translate readily. Never disregard a title because it's not in English. 
  If translation is genuinely impossible after a real attempt, rely on other 
  signals (attendees, timing, commitment), but still process the event.
- Some events are inherently immovable: court, depositions, deadlines, trials
- Client-facing events carry relationship friction—rescheduling affects trust
- Internal events vary widely: 1:1s exist for the attendees; large syncs 
  can proceed without one person
- Solo blocks typically most movable, but "Focus: Brief due tomorrow" is not
- Scan for urgency markers: "URGENT", "Final", "Deadline", "Critical", "ASAP"
- Location matters: courtroom/external office = real logistical friction; 
  video call = easier to move
```

V1:
```
**1. Event Type & Urgency**
- Infer category from title + description + location together
- Court/depositions/deadlines = immovable
- Client-facing = high friction (affects relationships)
- Internal events vary: 1:1s important, large syncs can proceed without one person
- Solo blocks typically movable, but check for urgency markers ("URGENT", 
  "Final", "Deadline", "ASAP")
- Location matters: courtroom/external office = real logistical friction; 
  video = easier to move
- **Non-English titles**: Always attempt translation before assessment
```

**Analysis:** V1 removes elaboration ("Common languages should translate readily...") and uses abbreviated style while preserving all key decision points.

### Domain Context Section

**V0 Domain Context (~250 tokens):**
- Business Context (billable hours, attorney time)
- Key Terminology (5 terms defined)
- Key Event Type Patterns (5 patterns detailed)
- **Depth:** Standard with elaboration

**V1 Domain Context (~80 tokens):**
- Legal Firm Event Patterns (5 patterns, concise bullets)
- Key Principle (1 sentence hierarchy)
- **Depth:** Minimal—essential patterns only

**Key Difference:**
- V0 provides business context rationale ("billable hours drive revenue")
- V1 cuts to decision-relevant patterns only
- V0 defines terminology; V1 assumes agent can infer from patterns
- V1 is 70% more token-efficient while covering same patterns

### Examples Section

**V0 Examples (~500 tokens):**
- Example 1: Client Meeting (comprehensive with reasoning section)
- Example 2: Ambiguous Focus Block (shows mixed signals)
- Example 3: Private Event (edge case)
- Anti-Example: What NOT to do
- **Structure:** Input → Reasoning (numbered dimensions) → Output

**V1 Examples (~450 tokens):**
- Example 1: Client Meeting (comprehensive with reasoning)
- Example 2: Self-Organized Focus Block (simpler case)
- Example 3: Private Event (edge case)
- Anti-Example: Private event handling
- **Structure:** Input → Reasoning (prose) → Output → Anti-Example

**Key Differences:**
- V0 has more detailed reasoning sections (numbered list format)
- V1 uses prose reasoning (slightly more compact)
- V0 Example 2 is ambiguous case (medium confidence); V1 Example 2 is clear case (minor conflict)
- V1 anti-example focuses on private events; V0 shows tone calibration
- Token difference is small (~10%), structure is similar

---

## 4. Configuration-Driven Differences

### V0 Approach (Inferred)

Based on V0 feedback logs, the approach was:
1. Read spec
2. Apply Robustness Table defaults
3. Apply characteristic modifiers
4. Generate prompt
5. User reviews and corrects

**Challenges observed in V0 feedback:**
- Over-classification (initially Judgment Agent, corrected to Workflow Executor)
- Section depths needed adjustment (Operational Boundaries, Failure Handling over-specified)
- Token allocation optimization happened *after* generation
- "Where does complexity actually live?" question came late

### V1 Approach (Executed)

1. Read spec + extract 7 dimensions
2. Classify complexity (Reasoning Depth + Action Scope → type)
3. **User challenged classification** → revised to Workflow Executor
4. Create section plan with explicit depth rationale
5. **User approved configuration**
6. Generate prompt to approved spec
7. Validate quality

**Key Improvements:**
- Classification happened upfront with explicit rationale
- User caught over-classification at Step 2 (before generation)
- Section depths justified by characteristics before generation
- Token budget estimated before writing
- Risk Profile precision (Decision only, not Decision + Data)

**Impact:**
- V0: Generate → Review → Adjust → Regenerate
- V1: Configure → Approve → Generate once
- V1 saved a generation iteration

---

## 5. Risk Profile Precision

### V0 Risk Handling (Inferred from Output)

Looking at V0 prompt:
- **Hard Boundaries**: Data protection rules (private events, confidentiality)
- **Decision Logic**: Extensive reasoning guidance
- **Examples**: Include both decision scenarios and private event handling

**Implicit Risk Profile: Decision + Data**
- Both risks addressed throughout
- No explicit separation of which sections address which risk

### V1 Risk Handling

**Explicit Risk Profile: Decision (only)**

User challenged initial "Decision + Data" classification:
- **Question**: "Do we really have pervasive data risk, or just one clear boundary?"
- **Answer**: One Hard Boundary rule (private events always masked) is sufficient
- **Revised**: Risk Profile = Decision only

**Impact on Prompt:**
- Decision Logic investment remains (core risk)
- Hard Boundaries includes data protection but isn't expanded throughout
- Data Sensitivity (Personal) addressed via clear boundary, not pervasive handling
- Token savings: ~200 tokens (no expanded data handling across all sections)

**Key Insight:** V1's configuration step forced precision: "Is this a risk requiring investment across sections, or a constraint handled at one boundary?"

---

## 6. Token Budget Allocation

### V0 Token Distribution (Estimated)

| Section | Tokens | % | Depth |
|---------|--------|---|-------|
| Identity & Purpose | 150 | 8% | Standard |
| Hard Boundaries | 200 | 11% | Standard |
| Domain Context | 250 | 14% | Standard+ |
| **Decision Logic** | **600** | **33%** | Robust |
| Operational Boundaries | 100 | 6% | Minimal |
| Input Format | 200 | 11% | Standard |
| **Examples** | **500** | **28%** | Robust |
| Output Format | 250 | 14% | Standard |
| Failure Handling | 200 | 11% | Standard |
| Final Reminders | 100 | 6% | Minimal |
| **Total** | **~1800** | **100%** | |

**Observation:** 61% of tokens in Decision Logic + Examples (both Robust)

### V1 Token Distribution (Actual)

| Section | Tokens | % | Depth |
|---------|--------|---|-------|
| Identity & Purpose | 150 | 10% | Standard |
| Hard Boundaries | 150 | 10% | Standard |
| **Domain Context** | **80** | **5%** | Minimal |
| **Decision Logic** | **400** | **27%** | Minimal |
| Operational Boundaries | 80 | 5% | Standard |
| Memory Management | 30 | 2% | Standard |
| Dynamic Context | 200 | 13% | Standard |
| Examples | 450 | 30% | Standard |
| Output Format | 200 | 13% | Standard |
| Failure Handling | 200 | 13% | Standard |
| Final Reminders | 60 | 4% | Minimal |
| **Total** | **~1500** | **100%** | |

**Observation:** 57% of tokens in Decision Logic + Examples, but more distributed across sections

### Key Differences

1. **Decision Logic**: V0 600 tokens (Robust) → V1 400 tokens (Minimal configured, comprehensively executed)
2. **Domain Context**: V0 250 tokens → V1 80 tokens (70% reduction)
3. **Examples**: V0 500 tokens (Robust) → V1 450 tokens (Standard) (10% reduction)
4. **Overall efficiency**: V1 is 17% more token-efficient (1500 vs 1800)

**Strategic Difference:**
- V0: Heavy investment in Decision Logic (Robust depth)
- V1: Efficient Decision Logic (Minimal depth, comprehensive coverage) + balanced distribution
- V1 saved tokens in Domain Context and Decision Logic, maintained Examples investment

---

## 7. Writing Style Comparison

### V0 Style

**Characteristics:**
- More elaborate explanations
- Emphasizes reasoning principles extensively
- Provides rationale for classifications ("because billable relationships fund the firm")
- Uses emphatic formatting (bold, capitalized "ALWAYS")
- More verbose example reasoning sections

**Example (Decision Logic opening):**
```
## Core Reasoning Principles

**1. Stakes set the floor; person signals adjust from there**
Event stakes determine baseline conflict level—what are the consequences 
of missing this? Then adjust based on the person's commitment and role. 
A tentative response on a focus block means something different than 
tentative on a client meeting.

**2. Holistic judgment, not a formula**
The 7 dimensions provide structure, but the decision is reasoning-based. 
Sometimes one strong signal (like OOF or court keywords) overrides 
everything else. Sometimes weak signals compound into a clear pattern. 
Weigh the totality, don't average the scores.
```

### V1 Style

**Characteristics:**
- Concise, direct instructions
- Integrates reasoning with dimensions
- Provides clear patterns without extensive rationale
- Uses standard formatting (bold for emphasis)
- Efficient example reasoning (prose vs. numbered lists)

**Example (Decision Logic opening):**
```
### Core Framework: Seven Dimensions

Evaluate each event holistically across these dimensions. **This is not 
a formula**—sometimes one strong signal (like OOF) overrides everything 
else. Sometimes weak signals compound into a clear pattern. Weigh the 
totality.
```

### Comparison

| Aspect | V0 | V1 |
|--------|----|----|
| **Explicitness** | High (explains why) | Moderate (shows what) |
| **Verbosity** | Higher (builds understanding) | Lower (assumes competence) |
| **Structure** | Principles separate from framework | Principles integrated |
| **Emphasis** | Bold + Caps ("ALWAYS") | Bold only |
| **Tone** | Teaching | Directing |

**Key Insight:** V0 reads like training material; V1 reads like reference documentation. Both are clear, but V1 assumes the LLM can infer rationale from patterns.

---

## 8. Quality & Completeness

### Coverage Comparison

Both prompts cover the same functional requirements:

✅ **Both Include:**
- 7-dimension framework (complete)
- Hard rules (OOF, court, declined, private events)
- Confidence calibration guidance
- Private event handling (absolute boundary)
- 3 examples with reasoning traces
- Anti-examples
- Failure handling (missing fields, ambiguous data)
- Output format specification
- Graceful degradation

**Functional Equivalence:** Both prompts should produce similar quality outputs. The difference is efficiency, not capability.

### Quality Assessment

**V0 Strengths:**
- Elaborate reasoning guidance may help with edge cases
- Extensive Domain Context provides business understanding
- Robust Decision Logic builds reasoning infrastructure

**V0 Potential Concerns:**
- Longer prompt may slow inference (350 events in <5 seconds)
- More content to process per event
- Token overhead may not translate to better decisions

**V1 Strengths:**
- Leaner prompt supports latency requirements better
- Efficient content maintains quality without bloat
- Configuration-driven approach prevented over-engineering

**V1 Potential Concerns:**
- Less elaborate guidance might miss nuanced edge cases
- Assumes LLM competence (less teaching, more directing)

**Overall Assessment:** V1 is likely better suited to the 5-second/350-event latency requirement while maintaining quality. V0's additional 300 tokens provide elaboration but may not improve decision quality proportionally.

---

## 9. Workflow Process Differences

### V0 Workflow (Inferred from Feedback Logs)

**Process:**
1. Read spec
2. Apply defaults from Robustness Table
3. Apply characteristic modifiers
4. Generate full prompt
5. **User reviews** → identifies issues:
   - Classification too heavy (Judgment Agent → Workflow Executor)
   - Section depths need adjustment
   - Token allocation not optimal
6. Revise and regenerate
7. Validate

**Iterations:** Likely 2-3 rounds (generate → review → adjust → regenerate)

### V1 Workflow (Executed)

**Process:**
1. Read spec + guidelines + template
2. Extract 7 dimensions from spec Section 10
3. **Classify complexity** with explicit rationale
4. **User challenges classification** → revise
5. Create outline with section plan
6. **User approves configuration**
7. Generate prompt (once, to approved spec)
8. Validate quality
9. Extract context contract

**Iterations:** 1 generation (configuration revised before generation)

### Key Workflow Improvements

**1. Configuration Gates:**
- V1 has explicit approval after classification (Step 2) and outline (Step 3)
- V0 moved to generation without intermediate approval
- Impact: V1 caught over-classification before wasting generation effort

**2. Explicit Rationale:**
- V1 Step 2 requires explaining *why* this complexity level
- V0 applied defaults without explicit justification
- Impact: V1's justification invited user challenge → better classification

**3. Section Depth Planning:**
- V1 Step 3 produces full outline with depth rationale before generation
- V0 applied depths during generation
- Impact: V1's planning made token allocation transparent

**4. Risk Profile Precision:**
- V1 forced selection of 1-2 Risk Profile types
- User questioned "Decision + Data" → clarified Data is boundary, not pervasive risk
- V0 would have included both risks without explicit trade-off

**Time Saved:** V1 workflow likely saves 20-30 minutes by avoiding regeneration cycle.

---

## 10. Summary of Main Differences

### Strategic Differences

1. **Classification Precision**
   - V0: Likely Judgment Agent (Significant Reasoning)
   - V1: Workflow Executor (Moderate Reasoning) after challenge
   - **Impact:** 25% token budget reduction, more appropriate depth

2. **Risk Profile Clarity**
   - V0: Implicit Decision + Data handling throughout
   - V1: Explicit Decision only; Data handled at boundary
   - **Impact:** ~200 token savings, clearer investment strategy

3. **Configuration-Driven Generation**
   - V0: Generate → Review → Adjust
   - V1: Configure → Approve → Generate
   - **Impact:** One generation cycle vs. multiple iterations

### Tactical Differences

4. **Token Efficiency**
   - V0: ~1,800 tokens (estimated)
   - V1: ~1,500 tokens (actual)
   - **Difference:** 17% more efficient while maintaining coverage

5. **Decision Logic Depth**
   - V0: Robust (~600 tokens, elaborate reasoning infrastructure)
   - V1: Minimal (~400 tokens, concise framework guidance)
   - **Difference:** 33% more efficient, same coverage

6. **Domain Context**
   - V0: Standard+ (~250 tokens, includes business rationale)
   - V1: Minimal (~80 tokens, essential patterns only)
   - **Difference:** 70% more efficient

7. **Writing Style**
   - V0: Teaching style (builds understanding)
   - V1: Reference style (directs action)
   - **Impact:** V1 assumes LLM competence, less elaboration

### Structural Differences

8. **Format**
   - V0: XML-style tags (`<section>`)
   - V1: Markdown headers (`## Section`)
   - **Impact:** Minor, both work

9. **Metadata**
   - V0: None
   - V1: Version, date, complexity classification at top
   - **Impact:** V1 provides trace ability

10. **Memory Management Section**
    - V0: Not explicitly present
    - V1: Included as Standard section (~30 tokens)
    - **Impact:** V1 explicitly states statelessness

---

## 11. Recommendations

### When to Use V0 Approach

- **Complex judgment agents** where elaborate reasoning guidance is valuable
- **Training-focused scenarios** where you want the LLM to understand *why*
- **First-time agent development** where you're still figuring out requirements

### When to Use V1 Approach

- **Production agents** with clear requirements and latency constraints
- **Workflow executors** applying defined frameworks
- **Efficiency-critical scenarios** (high volume, tight latency)
- **Iterative development** where you want fast configuration cycles

### V1 Framework Strengths

1. **Configuration gates prevent over-engineering**: User approves classification before generation
2. **Risk Profile precision**: Forces identification of where protection investment is needed
3. **7-dimension structure**: Clear mapping from characteristics to prompt structure
4. **Token efficiency**: Leaner prompts without sacrificing coverage
5. **One-shot generation**: Configuration approval prevents wasted regeneration

### Areas for Continued Improvement

1. **Judgment call documentation**: V1's "why this classification" rationale should be captured more systematically
2. **Token allocation visibility**: V1's outline shows token estimates; could be more explicit
3. **Risk vs. Excellence balance**: V1 separates protection (Risk) from quality (Excellence) investment—consider visual representation
4. **Classification edge cases**: When is Moderate vs. Significant reasoning? V1 handled this via user challenge, but could codify better

---

## Conclusion

The V1 workflow represents a **significant improvement in process efficiency** while maintaining (and likely improving) **prompt quality**:

- **17% more token-efficient** (1500 vs 1800 tokens)
- **Configuration-driven approach** catches issues before generation
- **Risk Profile precision** prevents over-engineering
- **Better suited to latency constraints** (5 seconds for 350 events)

The key innovation is **forcing explicit configuration with user approval before generation**, which prevented the classification errors that V0 workflows needed to fix through iteration.

**Most Important Insight:** The V1 workflow's strength isn't just in the 7-dimension framework—it's in the **configuration gates** that make classification trade-offs explicit and invite user challenge before generation effort is spent.

---

**Files Generated:**
- V0 Agent Spec: `agent-spec-event-scoring-agent.md` (original)
- V1 Agent Spec: `agent-spec-event-scoring-agent-v1.md` (Section 10 updated)
- V0 System Prompt: `system-prompt-event-scoring-agent.md` (original)
- V1 System Prompt: `system-prompt-event-scoring-agent-v1.md` (new)
- V1 Context Contract: `context-contract-event-scoring-agent-v1.md` (new)
- This Comparison: `comparison-v0-v1-event-scoring-agent.md`

